{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## train流程\n",
    "## upernet\n",
    "因为涉及到多数据集加载和"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# System libs\n",
    "import os\n",
    "import time\n",
    "# import math\n",
    "import random\n",
    "# Numerical libs\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.initialized = False\n",
    "        self.val = None\n",
    "        self.avg = None\n",
    "        self.sum = None\n",
    "        self.count = None\n",
    "\n",
    "    def initialize(self, val, weight):\n",
    "        self.val = val\n",
    "        self.avg = val\n",
    "        self.sum = val * weight\n",
    "        self.count = weight\n",
    "        self.initialized = True\n",
    "\n",
    "    def update(self, val, weight=1):\n",
    "        if not self.initialized:\n",
    "            self.initialize(val, weight)\n",
    "        else:\n",
    "            self.add(val, weight)\n",
    "\n",
    "    def add(self, val, weight):\n",
    "        self.val = val\n",
    "        self.sum += val * weight\n",
    "        self.count += weight\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def value(self):\n",
    "        return self.val\n",
    "\n",
    "    def average(self):\n",
    "        return self.avg\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizers, cur_iter, args):\n",
    "    scale_running_lr = ((1. - float(cur_iter) / args.max_iters) ** args.lr_pow)\n",
    "    args.running_lr_encoder = args.lr_encoder * scale_running_lr\n",
    "    args.running_lr_decoder = args.lr_decoder * scale_running_lr\n",
    "\n",
    "    (optimizer_encoder, optimizer_decoder) = optimizers\n",
    "    for param_group in optimizer_encoder.param_groups:\n",
    "        param_group['lr'] = args.running_lr_encoder\n",
    "    for param_group in optimizer_decoder.param_groups:\n",
    "        param_group['lr'] = args.running_lr_decoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train(segmentation_module, iterator, optimizers, history, epoch, args):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "\n",
    "    names = ['object', 'part', 'scene', 'material']\n",
    "    ave_losses = {n: AverageMeter() for n in names}\n",
    "    ave_metric = {n: AverageMeter() for n in names}\n",
    "    ave_losses['total'] = AverageMeter()\n",
    "\n",
    "    # segmetation_module 为网络加载结构\n",
    "    segmentation_module.train(not args.fix_bn)\n",
    "\n",
    "    # main loop\n",
    "    tic = time.time()\n",
    "    for i in range(args.epoch_iters):\n",
    "        # next 方法，返回迭代器的下个使用\n",
    "        batch_data, src_idx = next(iterator)\n",
    "        data_time.update(time.time() - tic)\n",
    "        segmentation_module.zero_grad()\n",
    "\n",
    "        # 前向传播\n",
    "        ret = segmentation_module(batch_data)\n",
    "\n",
    "        # 计算反向传播\n",
    "        loss = ret['loss']['total'].mean()\n",
    "        loss.backward()\n",
    "        for optimizer in optimizers:\n",
    "            optimizer.step()\n",
    "\n",
    "        # 计算剩余时间\n",
    "        batch_time.update(time.time()-tic)\n",
    "        tic = time.time()\n",
    "\n",
    "        # measure losses\n",
    "        for name in ret['loss'].keys():\n",
    "            ave_losses[name].update(ret['loss'][name].mean().item())\n",
    "\n",
    "        # measure metrics\n",
    "        for name in ret['metric'].keys():\n",
    "            ave_metric[name].update(ret['metric'][name].mean().item())\n",
    "\n",
    "        # calculate acc and display: logging output\n",
    "        if i % args.disp_iter == 0:\n",
    "            loss_info = \"Loss: total {:.4f}, \".format(ave_losses['total'].average())\n",
    "            loss_info += \", \".join([\"{} {:.2f}\".format(\n",
    "                n[0], ave_losses[n].average()\n",
    "                if ave_losses[n].average() is not None else 0) for n in names])\n",
    "            acc_info = \"Accuracy: \" + \", \".join([\"{} {:4.2f}\".format(\n",
    "                n[0], ave_metric[n].average()\n",
    "                if ave_metric[n].average() is not None else 0) for n in names])\n",
    "            print('Epoch: [{}][{}/{}], Time: {:.2f}, Data: {:.2f}, '\n",
    "                  'LR: encoder {:.6f}, decoder {:.6f}, {}, {}'\n",
    "                  .format(epoch, i, args.epoch_iters,\n",
    "                          batch_time.average(), data_time.average(),\n",
    "                          args.running_lr_encoder, args.running_lr_decoder,\n",
    "                          acc_info, loss_info))\n",
    "\n",
    "            fractional_epoch = epoch - 1 + 1. * i / args.epoch_iters\n",
    "            history['train']['epoch'].append(fractional_epoch)\n",
    "            history['train']['loss'].append(loss.item())\n",
    "\n",
    "        # adjust learning rate\n",
    "        cur_iter = i + (epoch - 1) * args.epoch_iters\n",
    "        adjust_learning_rate(optimizers, cur_iter, args)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 创建保存点\n",
    "这里是每一个epoch一个保存点，首先载入"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def checkpoint(nets, history, args, epoch_num):\n",
    "    print('Saving checkpoints')\n",
    "    (net_encoder, net_decoder) = nets\n",
    "    suffix_latest = 'epoch_{}.pth'.format(epoch_num)\n",
    "\n",
    "    dict_encoder = net_encoder.state_dict()\n",
    "    dict_decoder = net_decoder.state_dict()\n",
    "\n",
    "    torch.save(history, '{}/history_{}'.format(args.ckpt, suffix_latest))\n",
    "    torch.save(dict_encoder, '{}/encoder_{}'.format(args.ckpt, suffix_latest))\n",
    "    torch.save(dict_decoder, '{}/decoder_{}'.format(args.ckpt, suffix_latest))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 创建optimizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def group_weight(module):\n",
    "    group_decay = []\n",
    "    group_no_decay = []\n",
    "    for m in module.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            group_decay.append(m.weight)\n",
    "            if m.bias is not None:\n",
    "                group_no_decay.append(m.bias)\n",
    "        elif isinstance(m, nn.modules.conv._ConvNd):\n",
    "            group_decay.append(m.weight)\n",
    "            if m.bias is not None:\n",
    "                group_no_decay.append(m.bias)\n",
    "        elif isinstance(m, nn.modules.batchnorm._BatchNorm):\n",
    "            if m.weight is not None:\n",
    "                group_no_decay.append(m.weight)\n",
    "            if m.bias is not None:\n",
    "                group_no_decay.append(m.bias)\n",
    "\n",
    "    assert len(list(module.parameters())) == len(group_decay) + len(group_no_decay)\n",
    "    groups = [dict(params=group_decay), dict(params=group_no_decay, weight_decay=.0)]\n",
    "    return groups"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_optimizers(nets, args):\n",
    "    (net_encoder, net_decoder) = nets\n",
    "    optimizer_encoder = torch.optim.SGD(\n",
    "        group_weight(net_encoder),\n",
    "        lr=args.lr_encoder,\n",
    "        momentum=args.beta1,\n",
    "        weight_decay=args.weight_decay)\n",
    "    optimizer_decoder = torch.optim.SGD(\n",
    "        group_weight(net_decoder),\n",
    "        lr=args.lr_decoder,\n",
    "        momentum=args.beta1,\n",
    "        weight_decay=args.weight_decay)\n",
    "    return (optimizer_encoder, optimizer_decoder)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 创建一个多重数据集的加载器\n",
    "其实我觉得是可以根据个人不同的东西"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_multi_source_train_data_loader(args):\n",
    "    training_records = broden_dataset.record_list['train']\n",
    "\n",
    "    # 0: object, part, scene\n",
    "    # 1: material\n",
    "    multi_source_iters = []\n",
    "    for idx_source in range(len(training_records)):\n",
    "        # def __init__(self, records, source_idx, opt, max_sample=-1, batch_per_gpu=1):\n",
    "        dataset = TrainDataset(training_records[idx_source], idx_source, args,\n",
    "                               batch_per_gpu=args.batch_size_per_gpu)\n",
    "        loader_object_part_scene = torchdata.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=args.num_gpus,  # we have modified data_parallel\n",
    "            shuffle=False,  # we do not use this param\n",
    "            collate_fn=user_scattered_collate,\n",
    "            num_workers=int(args.workers),\n",
    "            drop_last=True,\n",
    "            pin_memory=True)\n",
    "        multi_source_iters.append(iter(loader_object_part_scene))\n",
    "\n",
    "    # sample from multi source\n",
    "    nr_record = [len(records) for records in training_records]\n",
    "    sample_prob = np.asarray(nr_record) / np.sum(nr_record)\n",
    "    while True:  # TODO(LYC):: set random seed.\n",
    "        source_idx = np.random.choice(len(training_records), 1, p=sample_prob)[0]\n",
    "        yield next(multi_source_iters[source_idx]), source_idx"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## main\n",
    "这里是已经封装好的main代码"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    builder = ModelBuilder()\n",
    "    # 载入网络结构\n",
    "    net_encoder = builder.build_encoder(\n",
    "        arch = args.arch_encoder,\n",
    "        fc_dim = args.fc_dim,\n",
    "        weights = args.weights_encoder\n",
    "    )\n",
    "    net_decoder = builder.build_encoder(\n",
    "        arch = args.fc_dim,\n",
    "        fc_dim = args.fc_dim,\n",
    "        nr_classes = args.nr_classes,\n",
    "        weights = args.weights_encoder\n",
    "    )\n",
    "\n",
    "    if args.arch_decoder.endswith('deepsup'):\n",
    "        segmentation_module = SegmentationModule(net_encoder, net_decoder, args.deep_sup_scale)\n",
    "    else:\n",
    "        segmentation_module = SegmentationModule(net_encoder, net_decoder)\n",
    "\n",
    "    print('1 Epoch = {} iters'.format(args.epoch_iters))\n",
    "\n",
    "    # create loader iterator\n",
    "    iterator_train = create_multi_source_train_data_loader(args=args)\n",
    "\n",
    "    # load nets into gpu\n",
    "    if args.num_gpus > 1:\n",
    "        segmentation_module = UserScatteredDataParallel(\n",
    "            segmentation_module,\n",
    "            device_ids=range(args.num_gpus))\n",
    "        # For sync bn\n",
    "        patch_replication_callback(segmentation_module)\n",
    "    segmentation_module.cuda()\n",
    "\n",
    "    # Set up optimizers\n",
    "    nets = (net_encoder, net_decoder)\n",
    "    optimizers = create_optimizers(nets, args)\n",
    "\n",
    "    # Main loop\n",
    "    history = {'train': {'epoch': [], 'loss': [], 'acc': []}}\n",
    "\n",
    "    for epoch in range(args.start_epoach, args.num_epoch + 1):\n",
    "        train(segmentation_module, iterator_train, optimizers, history, epoch, args)\n",
    "        checkpoint(nets, history, args, epoch)\n",
    "\n",
    "    print('Training done')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}