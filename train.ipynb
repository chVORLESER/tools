{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## train流程\n",
    "## upernet\n",
    "因为涉及到多数据集加载和"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# System libs\n",
    "import os\n",
    "import time\n",
    "# import math\n",
    "import random\n",
    "# Numerical libs\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train(segmentation_module, iterator, optimizers, history, epoch, args):\n",
    "    batch_time = AverageMeter()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 创建保存点\n",
    "这里是每一个epoch一个保存点，首先载入"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def checkpoint(nets, history, args, epoch_num):\n",
    "    print('Saving checkpoints')\n",
    "    (net_encoder, net_decoder) = nets\n",
    "    suffix_latest = 'epoch_{}.pth'.format(epoch_num)\n",
    "\n",
    "    dict_encoder = net_encoder.state_dict()\n",
    "    dict_decoder = net_decoder.state_dict()\n",
    "\n",
    "    torch.save(history, '{}/history_{}'.format(args.ckpt, suffix_latest))\n",
    "    torch.save(dict_encoder, '{}/encoder_{}'.format(args.ckpt, suffix_latest))\n",
    "    torch.save(dict_decoder, '{}/decoder_{}'.format(args.ckpt, suffix_latest))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 创建optimizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_optimizers(nets, args):\n",
    "    (net_encoder, net_decoder) = nets\n",
    "    optimizer_encoder = torch.optim.SGD(\n",
    "        group_weight(net_encoder),\n",
    "        lr=args.lr_encoder,\n",
    "        momentum=args.beta1,\n",
    "        weight_decay=args.weight_decay)\n",
    "    optimizer_decoder = torch.optim.SGD(\n",
    "        group_weight(net_decoder),\n",
    "        lr=args.lr_decoder,\n",
    "        momentum=args.beta1,\n",
    "        weight_decay=args.weight_decay)\n",
    "    return (optimizer_encoder, optimizer_decoder)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 创建一个多重数据集的加载器\n",
    "其实我觉得是可以根据个人不同的东西"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_multi_source_train_data_loader(args):\n",
    "    training_records = broden_dataset.record_list['train']\n",
    "\n",
    "    # 0: object, part, scene\n",
    "    # 1: material\n",
    "    multi_source_iters = []\n",
    "    for idx_source in range(len(training_records)):\n",
    "        # def __init__(self, records, source_idx, opt, max_sample=-1, batch_per_gpu=1):\n",
    "        dataset = TrainDataset(training_records[idx_source], idx_source, args,\n",
    "                               batch_per_gpu=args.batch_size_per_gpu)\n",
    "        loader_object_part_scene = torchdata.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=args.num_gpus,  # we have modified data_parallel\n",
    "            shuffle=False,  # we do not use this param\n",
    "            collate_fn=user_scattered_collate,\n",
    "            num_workers=int(args.workers),\n",
    "            drop_last=True,\n",
    "            pin_memory=True)\n",
    "        multi_source_iters.append(iter(loader_object_part_scene))\n",
    "\n",
    "    # sample from multi source\n",
    "    nr_record = [len(records) for records in training_records]\n",
    "    sample_prob = np.asarray(nr_record) / np.sum(nr_record)\n",
    "    while True:  # TODO(LYC):: set random seed.\n",
    "        source_idx = np.random.choice(len(training_records), 1, p=sample_prob)[0]\n",
    "        yield next(multi_source_iters[source_idx]), source_idx"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## main\n",
    "这里是已经封装好的main代码"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    builder = ModelBuilder()\n",
    "    # 载入网络结构\n",
    "    net_encoder = builder.build_encoder(\n",
    "        arch = args.arch_encoder,\n",
    "        fc_dim = args.fc_dim,\n",
    "        weights = args.weights_encoder\n",
    "    )\n",
    "    net_decoder = builder.build_encoder(\n",
    "        arch = args.fc_dim,\n",
    "        fc_dim = args.fc_dim,\n",
    "        nr_classes = args.nr_classes,\n",
    "        weights = args.weights_encoder\n",
    "    )\n",
    "\n",
    "    if args.arch_decoder.endswith('deepsup'):\n",
    "        segmentation_module = SegmentationModule(net_encoder, net_decoder, args.deep_sup_scale)\n",
    "    else:\n",
    "        segmentation_module = SegmentationModule(net_encoder, net_decoder)\n",
    "\n",
    "    print('1 Epoch = {} iters'.format(args.epoch_iters))\n",
    "\n",
    "    # create loader iterator\n",
    "    iterator_train = create_multi_source_train_data_loader(args=args)\n",
    "\n",
    "    # load nets into gpu\n",
    "    if args.num_gpus > 1:\n",
    "        segmentation_module = UserScatteredDataParallel(\n",
    "            segmentation_module,\n",
    "            device_ids=range(args.num_gpus))\n",
    "        # For sync bn\n",
    "        patch_replication_callback(segmentation_module)\n",
    "    segmentation_module.cuda()\n",
    "\n",
    "    # Set up optimizers\n",
    "    nets = (net_encoder, net_decoder)\n",
    "    optimizers = create_optimizers(nets, args)\n",
    "\n",
    "    # Main loop\n",
    "    history = {'train': {'epoch': [], 'loss': [], 'acc': []}}\n",
    "\n",
    "    for epoch in range(args.start_epoach, args.num_epoch + 1):\n",
    "        train(segmentation_module, iterator_train, optimizers, history, epoach, args)\n",
    "        checkpoint(nets, history, args, epoch)\n",
    "\n",
    "    print('Training done')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}