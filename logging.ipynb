{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# logging\n",
    "## wechat_challenge setup_logging()\n",
    "logging类是非常重要的一种打印输出的手法。在深度学习中，需要打印的有acc,loss,time,等等参数\n",
    "最好按组输出训练参数，并且分组计算acc或者计算别的方法"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "\n",
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup_logging():\n",
    "    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                        datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                        level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# build_optimizer\n",
    "## wechat_challenge build_optimizer\n",
    "\n",
    "我个人一直有一个疑惑就是optimizer和训练的输出应当如何控制 如何去编写一个训练过程控制器\n",
    "- build_optimizer 类里卖弄包含了bias和layernorm.weight\n",
    "- optimizer使用了AdamW初始化 scheduler\n",
    "- 返回optimizer和scheduler"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "def build_optimizer(args, model):\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay': args.weight_decay},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "         'weight': 0.0}\n",
    "    ]\n",
    "    # 在这里使用了transformer初始方式\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps,\n",
    "                                                num_training_steps=args.max_steps)\n",
    "    return optimizer, scheduler"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# AdamW 初始方式函数\n",
    "class AdamW(Optimizer):\n",
    "    \"\"\"\n",
    "    Implements Adam algorithm with weight decay fix as introduced in [Decoupled Weight Decay\n",
    "    Regularization](https://arxiv.org/abs/1711.05101).\n",
    "\n",
    "    Parameters:\n",
    "        params (`Iterable[nn.parameter.Parameter]`):\n",
    "            Iterable of parameters to optimize or dictionaries defining parameter groups.\n",
    "        lr (`float`, *optional*, defaults to 1e-3):\n",
    "            The learning rate to use.\n",
    "        betas (`Tuple[float,float]`, *optional*, defaults to (0.9, 0.999)):\n",
    "            Adam's betas parameters (b1, b2).\n",
    "        eps (`float`, *optional*, defaults to 1e-6):\n",
    "            Adam's epsilon for numerical stability.\n",
    "        weight_decay (`float`, *optional*, defaults to 0):\n",
    "            Decoupled weight decay to apply.\n",
    "        correct_bias (`bool`, *optional*, defaults to `True`):\n",
    "            Whether or not to correct bias in Adam (for instance, in Bert TF repository they use `False`).\n",
    "        no_deprecation_warning (`bool`, *optional*, defaults to `False`):\n",
    "            A flag used to disable the deprecation warning (set to `True` to disable the warning).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params: Iterable[nn.parameter.Parameter],\n",
    "        lr: float = 1e-3,\n",
    "        betas: Tuple[float, float] = (0.9, 0.999),\n",
    "        eps: float = 1e-6,\n",
    "        weight_decay: float = 0.0,\n",
    "        correct_bias: bool = True,\n",
    "        no_deprecation_warning: bool = False,\n",
    "    ):\n",
    "        if not no_deprecation_warning:\n",
    "            warnings.warn(\n",
    "                \"This implementation of AdamW is deprecated and will be removed in a future version. Use the\"\n",
    "                \" PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\",\n",
    "                FutureWarning,\n",
    "            )\n",
    "        require_version(\"torch>=1.5.0\")  # add_ with alpha\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr} - should be >= 0.0\")\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(f\"Invalid beta parameter: {betas[0]} - should be in [0.0, 1.0)\")\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(f\"Invalid beta parameter: {betas[1]} - should be in [0.0, 1.0)\")\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(f\"Invalid epsilon value: {eps} - should be >= 0.0\")\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, correct_bias=correct_bias)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure: Callable = None):\n",
    "        \"\"\"\n",
    "        Performs a single optimization step.\n",
    "\n",
    "        Arguments:\n",
    "            closure (`Callable`, *optional*): A closure that reevaluates the model and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError(\"Adam does not support sparse gradients, please consider SparseAdam instead\")\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state[\"step\"] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state[\"exp_avg\"] = torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state[\"exp_avg_sq\"] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n",
    "                beta1, beta2 = group[\"betas\"]\n",
    "\n",
    "                state[\"step\"] += 1\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                # In-place operations to update the averages at the same time\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=(1.0 - beta1))\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1.0 - beta2)\n",
    "                denom = exp_avg_sq.sqrt().add_(group[\"eps\"])\n",
    "\n",
    "                step_size = group[\"lr\"]\n",
    "                if group[\"correct_bias\"]:  # No bias correction for Bert\n",
    "                    bias_correction1 = 1.0 - beta1 ** state[\"step\"]\n",
    "                    bias_correction2 = 1.0 - beta2 ** state[\"step\"]\n",
    "                    step_size = step_size * math.sqrt(bias_correction2) / bias_correction1\n",
    "\n",
    "                p.data.addcdiv_(exp_avg, denom, value=-step_size)\n",
    "\n",
    "                # Just adding the square of the weights to the loss function is *not*\n",
    "                # the correct way of using L2 regularization/weight decay with Adam,\n",
    "                # since that will interact with the m and v parameters in strange ways.\n",
    "                #\n",
    "                # Instead we want to decay the weights in a manner that doesn't interact\n",
    "                # with the m/v parameters. This is equivalent to adding the square\n",
    "                # of the weights to the loss with plain (non-momentum) SGD.\n",
    "                # Add weight decay at the end (fixed version)\n",
    "                if group[\"weight_decay\"] > 0.0:\n",
    "                    p.data.add_(p.data, alpha=(-group[\"lr\"] * group[\"weight_decay\"]))\n",
    "\n",
    "        return loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "当然 上面的只是一个输出模板，具体输出过程会由training函数承担。会嵌入进循环里面\n",
    "这里有一个非常好的习惯，首先记录总步数，然后通过实际训练时间得到剩余时间\n",
    "\n",
    "### 训练代码的一些好习惯\n",
    "首先，我们要明确training的构成。training一般由dataloader \\ model \\ optimizer \\ scheduler \\ training step arrangement \\ validation arrangement \\ save checkpoint 组成。在微信挑战赛的代码里面，train_validation代码块和main做了一个分离。\n",
    "同时，在进入不同模块时应当有一个标注，标识出哪一段属于该节点。首先定义变量 按照dataloader \\ import model and optimizers (model\\optimizer\\args.device\\step\\best_score\\start_time\\num_total_time etc) \\ training \\ validation \\ save checkpoint 来进行。\n",
    "同时，validate代码可以首先写出来，然后train的时候用validate的代码块。evaluate定义在utils中，可以方便拆解和定义"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def validate(model, val_dataloader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            loss, _, pred_label_id, label = model(batch)\n",
    "            loss = loss.mean()\n",
    "            predictions.exend(pred_label_id.cpu().numpy())\n",
    "            labels.extend(label.cpu().numpy())\n",
    "            losses.extend(loss.cpu().numpy())\n",
    "    loss = sum(losses) / len(losses)\n",
    "    results = evaluate(predictions, labels)\n",
    "\n",
    "    model.train()\n",
    "    return loss, results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (Temp/ipykernel_1448/970595466.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  File \u001B[1;32m\"C:\\Users\\Peiyi\\AppData\\Local\\Temp/ipykernel_1448/970595466.py\"\u001B[1;36m, line \u001B[1;32m1\u001B[0m\n\u001B[1;33m    for epoch in range(args.best_score):\u001B[0m\n\u001B[1;37m                                        ^\u001B[0m\n\u001B[1;31mIndentationError\u001B[0m\u001B[1;31m:\u001B[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "from config import parse_args\n",
    "\n",
    "args = parse_args()\n",
    "model = []\n",
    "\n",
    "step = 0\n",
    "best_score = args.best_score\n",
    "start_time = time.time()\n",
    "num_total_steps = len(train_dataloader) * args.max_epochs\n",
    "\n",
    "optimizer, scheduler = build_optimizer(args, model)\n",
    "for epoch in range(args.max_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        model.train()\n",
    "        loss, accuracy, _, _ = model(batch)\n",
    "        loss = loss.mean()\n",
    "        accuracy = accuracy.mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        scheduler.step()\n",
    "\n",
    "        step += 1\n",
    "        if step % args.print_steps == 0:\n",
    "            time_per_step = (time.time() - start_time) / max(1, step)\n",
    "            remaining_time = time_per_step * (num_total_steps - step)\n",
    "            remaining_time = time.strftime('%H:%M:%S', time.gmtime(remaining_time))\n",
    "            logging.info(f\"Epoch {epoch} step {step} eta {remaining_time}: loss{loss:.3f} accuarcy{accuracy:.3f}\")\n",
    "\n",
    "        loss, results = validate(model, val_dataloader)\n",
    "        results = {k: round(v, 4) for k, v in results.items()}\n",
    "        logging.info(f\"Epoch {epoch} step {step}: loss {loss:.3f}, {results}\")\n",
    "\n",
    "        # 5. save checkpoint 保存点\n",
    "        mean_f1 = results['mean_f1']\n",
    "        # 这里的best_score没有进行迭代\n",
    "        if mean_f1 > best_score:\n",
    "            best_score = mean_f1\n",
    "            state_dict = model.module.state_dict() if args.device == 'cuda' else model.state_dict()\n",
    "            torch.save({'epoch': epoch, 'model_state_dict': state_dict, 'mean_f1': mean_f1},\n",
    "                       f'{args.savedmodel_path}/model_epoch_{epoch}_mean_f1_{mean_f1}.bin')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import time\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}